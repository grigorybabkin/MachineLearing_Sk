{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-T844HDTKad"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2022_seminars/blob/master/seminar5/Trees_Bagging_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ha8adx-dTKah"
   },
   "source": [
    "# Seminar: Trees, Bootstrap Aggregation (Bagging) and Random Forest\n",
    "Machine Learning by professor Evgeny Burnaev\n",
    "<br\\>\n",
    "Author: Andrey Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4aofO-_oTKai"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fM68yzJTKam"
   },
   "source": [
    "# Example 1: Regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "Pn8YBI0STKan",
    "outputId": "1514c45e-b1c3-466b-8477-cca1ef5c95b0"
   },
   "outputs": [],
   "source": [
    "# prepare and show a dataset\n",
    "n = 1                         # number of features\n",
    "N = 100**n                    # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*3\n",
    "coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))/3\n",
    "y = y.ravel()\n",
    "print((pd.DataFrame({'x1': X[:, 0], 'y': y})))\n",
    "plt.plot(X, y, '*')\n",
    "plt.title('1 predictor: x1; target: y')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# train and predict a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "plt.plot(X, y_pred, '.r')\n",
    "plt.show()\n",
    "\n",
    "print('Mean Squared Error: ', mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TARmhz1jTKaq"
   },
   "source": [
    "### Question 1.1: Change the number of levels in a regression tree above until the best approximation of the training set. What is the best MSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edrWTpBATKas"
   },
   "source": [
    "### Question 1.2: Calculate MSE above without `mean_squared_error()` calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovQfL2AvTKat"
   },
   "source": [
    "## Example 2: Regression tree with $n=2$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb3_gvdWTKau"
   },
   "source": [
    "### Question 2.1: Lets try an example with $n=2$ features. Train a regression stump (a tree of a depth 1) and see the optimal threshold (border between colors) for the best feature chosen for split among $x_1$ and $x_2$. What feature was chosen and why? Change something in the string \n",
    "```\n",
    "coeffs = np.array([[0.2], [1.5]])\n",
    "```\n",
    "### to make another feature is chosen as the best for split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9LEj01XTKav"
   },
   "source": [
    "### Question 2.2: Find the optimal `max_depth` hyperparameter when MSE on the training set is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "colab_type": "code",
    "id": "c73kkkA9TKav",
    "outputId": "2d5e57b0-0f87-46ee-e3ae-d7ad93777bb4"
   },
   "outputs": [],
   "source": [
    "# prepare and show a dataset in 2D\n",
    "n = 2               # number of features\n",
    "N = 100**n          # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*1.8\n",
    "\n",
    "# Change something to make another feature is chosen for split by some optimal threshold\n",
    "coeffs = np.array([[0.2], [1.5]])\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))/3\n",
    "\n",
    "# print dataset\n",
    "print((pd.DataFrame({'x1': X[:, 0], 'x2': X[:, 1], 'y': y.ravel()})))\n",
    "\n",
    "# show target y in (x1, x2) space\n",
    "plt.figure(figsize=[9, 6])\n",
    "sc = plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=5)\n",
    "plt.colorbar(sc)\n",
    "plt.title('training data: predictors x1, x2 and target y')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()\n",
    "\n",
    "# train and predict by a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=5)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# show prediction in (x1, x2) space\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=5)\n",
    "plt.colorbar(sc)\n",
    "plt.title('prediction on the training data')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()\n",
    "\n",
    "print('Mean Squared Error: ', mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uh-op1b-TKaz"
   },
   "source": [
    "## Example 3: Regression tree: training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nmo0dNQQTKa0"
   },
   "source": [
    "### Question 3.1: Now we consider training and testing sets. Try different depths of a decision tree to see when the model is underfitted and when the one is overfitted to the training set. Plot the MSE on the testing set depending on `max_depth` hyperparameter. What is the optimal value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "colab_type": "code",
    "id": "geXnQ2_GTKa0",
    "outputId": "d14083f2-5c64-4b53-ba3c-716c2e9f9e2d"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare dataset\n",
    "n = 1                         # number of features\n",
    "N = 200**n                    # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*3\n",
    "coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/2, random_state=0)\n",
    "\n",
    "# --- change this block to select the best max_depth\n",
    "clf = DecisionTreeRegressor(max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.plot(X_train, y_train, '*')\n",
    "plt.plot(X_train, clf.predict(X_train), '.r')\n",
    "plt.title('training dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.plot(X_test, y_test, '*')\n",
    "plt.plot(X_test, clf.predict(X_test), '.r')\n",
    "plt.title('testing dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8DthEy1Y-TY"
   },
   "source": [
    "### Question 3.2. How many constant-valued regions of red points are on the picture when `max_depth=5`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODCAU-cxTKa3"
   },
   "source": [
    "# Example 4: Bagging = Decision Tree + Bootstrap\n",
    "### In the question above we've found the optimal `max_depth` for the case of single Decision Tree. By limiting the tree depth we distort the fitting to the training dataset and prevent the model from overfitting. \n",
    "### The second way to prevent overfitting is to distort the ... training dataset itself. What is Bagging?\n",
    "### We train many trees each on a Bootstraped training dataset (it contains the same number of samples but some of them are included with some number of their copies, and some of them are not included). Then we average over all such trees. It is called Bootstrap aggregation - Bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bye47AOmTKa4"
   },
   "source": [
    "### Question 4.1: Compare the prediction above (single tree) with Bagging all with `max_depth=5`. Why Bagging approximation red dots does not look like constant-valued regions as in a single decision tree? Tune the best number of trees. Has  Bagging improved the single tree model regarding `test MSE`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7d8jyfDATKa5"
   },
   "source": [
    "### Question 4.2: Sometimes it is good to be a perfectionist and unittest any code :). Unittest scikit-learn! Should `DecisionTreeRegressor` and `BaggingRegressor`  give the same results in some special case? Simplify Bagging to the single Decision Tree and show the same pictures as above when `max_depth=5`. What parameter `n_estimators` have to be set to? Do we need to change any other hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "colab_type": "code",
    "id": "AsxzenebTKa5",
    "outputId": "c21e9e12-dab6-4614-95c4-e42ebc63ac69",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare dataset\n",
    "n = 1                         # number of features\n",
    "N = 200**n                    # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*3\n",
    "coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/2, random_state=0)\n",
    "\n",
    "# --- 1. change this block to select the best n_estimators\n",
    "# --- 2. change this block to simplify Bagging to ordinary single decision tree\n",
    "clf = BaggingRegressor(DecisionTreeRegressor(max_depth=12), n_estimators=10, bootstrap=True, random_state=0)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "# ---\n",
    "\n",
    "# plt.figure(figsize=[9, 6])\n",
    "plt.plot(X_train, y_train, '*')\n",
    "plt.plot(X_train, clf.predict(X_train), '.r')\n",
    "plt.title('training dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=[9, 6])\n",
    "plt.plot(X_test, y_test, '*')\n",
    "plt.plot(X_test, clf.predict(X_test), '.r')\n",
    "plt.title('testing dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_nx2p4NfCBo"
   },
   "source": [
    "### Question 4.3: Estimate the part of points that are not selected after Bootstrap procedure analytically. Consider the sample of a size $N\\to\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qujVV30ATKa9"
   },
   "source": [
    "# Example 5: Random Forest = Bagging + (`max_features' < $n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nXxzoGdTKa-"
   },
   "source": [
    "## Can we reduce the overfitting more? - YES!!! <br> Along with Bootstrap reduce the number of features among which the best feature for each tree in ensemble is chosen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-xYYau3TKa_"
   },
   "source": [
    "### Question 5.1: Tune `max_features` to improve the tesing MSE. Check that testing MSE becomes better. And what happens with training MSE and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "vF0oi4utTKa_",
    "outputId": "19f922e5-e6ed-4c72-d1d0-f40a32621215"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare dataset\n",
    "n = 100           # number of features\n",
    "N = 10**4         # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*3\n",
    "coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/2, random_state=0)\n",
    "\n",
    "# --- change this block to select the best max_features\n",
    "clf = RandomForestRegressor(max_depth=5, n_estimators=10, max_features=n, random_state=0)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest on Kaggle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below shorter trees and 'max_features' <  all features performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "X_train = pd.read_csv('https://raw.githubusercontent.com/adasegroup/ML2022_seminars/master/seminar5/give_me_some_credit.csv', index_col=0)\n",
    "X_train = X_train.dropna()\n",
    "y_train = X_train['SeriousDlqin2yrs']\n",
    "X_train = X_train.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "\n",
    "X_train = X_train.iloc[::10, :] # use only each 10th sample to save a time\n",
    "y_train = y_train.iloc[::10] \n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=0),\n",
    "                   # you can play with tuning, up to your CPU performance:\n",
    "                   {'max_depth': [10, 15], 'max_features': ['auto', n_features]},\n",
    "                   scoring = 'roc_auc',\n",
    "                   cv = 3, \n",
    "                   n_jobs=-1)\n",
    "# You can see below that detailed trees i.e. with high depth and all features on a split are worse, \n",
    "# GridSearchCV() choses shorter depth and sqrt() of all features.\n",
    "# This is because we use cross-validation (cv=3), and it prevents from overfitting\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('best parameters:', clf.best_params_)\n",
    "print('ROC_AUC score:', roc_auc_score(y_train, clf.predict(X_train)))\n",
    "\n",
    "# now let's draw ROC curve\n",
    "plt.figure(figsize=[9, 6])\n",
    "fpr, tpr, _ = roc_curve(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "plt.plot(fpr, tpr, 'r', label='train')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVEms_2pTKbC"
   },
   "source": [
    "# Example 6: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('https://raw.githubusercontent.com/adasegroup/ML2022_seminars/master/seminar5/give_me_some_credit.csv', index_col=0)\n",
    "y_train = X_train['SeriousDlqin2yrs']\n",
    "X_train = X_train.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "\n",
    "X_train = X_train.iloc[::10, :] # use only each 10th sample to save a time\n",
    "y_train = y_train.iloc[::10] \n",
    "\n",
    "clf = RandomForestClassifier(max_depth = 10, max_features = 'auto', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print('ROC_AUC score:', roc_auc_score(y_train, clf.predict(X_train)))\n",
    "\n",
    "# feature importances\n",
    "fi = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "fi.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('feature importances')\n",
    "plt.ylim([0, 0.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_3ZE7fUTKbI"
   },
   "source": [
    "### Be careful with `feature_importances`: after adding extremly correlated features (here they are even copies) the values of importance decrease (see the scale along vertical axis). This is expected, because similar features can share their common similar importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "colab_type": "code",
    "id": "W0PtboZXTKbJ",
    "outputId": "74415fbe-d62f-409e-92f4-4f82e888e797"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('https://raw.githubusercontent.com/adasegroup/ML2022_seminars/master/seminar5/give_me_some_credit.csv', index_col=0)\n",
    "X_train = X_train.dropna()\n",
    "y_train = X_train['SeriousDlqin2yrs']\n",
    "X_train = X_train.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "\n",
    "X_train = X_train.iloc[::10, :] # use only each 10th sample to save a time\n",
    "y_train = y_train.iloc[::10] \n",
    "\n",
    "X_train['NumberOfTimes90DaysLate_2'] = X_train['NumberOfTimes90DaysLate']\n",
    "X_train['NumberOfTimes90DaysLate_3'] = X_train['NumberOfTimes90DaysLate']\n",
    "X_train['NumberOfTimes90DaysLate_4'] = X_train['NumberOfTimes90DaysLate']\n",
    "X_train['NumberOfTimes90DaysLate_5'] = X_train['NumberOfTimes90DaysLate']\n",
    "\n",
    "clf = RandomForestClassifier(max_depth = 10, max_features = 'auto', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print('ROC_AUC score:', roc_auc_score(y_train, clf.predict(X_train)))\n",
    "\n",
    "# now let's draw ROC curve\n",
    "plt.figure(figsize=[9, 6])\n",
    "fpr, tpr, _ = roc_curve(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "plt.plot(fpr, tpr, 'r', label='train')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()\n",
    "\n",
    "# feature importances\n",
    "fi = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "fi.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('feature importances')\n",
    "plt.ylim([0, 0.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Feature Importance\n",
    "\n",
    "### Above, the feature 'NumberOfTimes90DaysLate' moved from the top to the tail together with its highly correlated (even cloned) 'friends'. Now they share their importance together. Do not think that top features are really strong and tail ones are weak! The can correlate significantly. \n",
    "\n",
    "### So, do not rely only on Feature Importance! Try to reduce the dimension, for. ex. by Feature Selection\n",
    "\n",
    "### In the following example if sequentially add the strongest feature one can recover the original feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "X_train = pd.read_csv('https://raw.githubusercontent.com/adasegroup/ML2022_seminars/master/seminar5/give_me_some_credit.csv', index_col=0)\n",
    "X_train = X_train.dropna()\n",
    "y_train = X_train['SeriousDlqin2yrs']\n",
    "X_train = X_train.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "\n",
    "X_train = X_train.iloc[::10, :] # use only each 10th sample to save a time\n",
    "y_train = y_train.iloc[::10] \n",
    "\n",
    "X_train['NumberOfTimes90DaysLate_2'] = X_train['NumberOfTimes90DaysLate']\n",
    "X_train['NumberOfTimes90DaysLate_3'] = X_train['NumberOfTimes90DaysLate']\n",
    "X_train['NumberOfTimes90DaysLate_4'] = X_train['NumberOfTimes90DaysLate']\n",
    "X_train['NumberOfTimes90DaysLate_5'] = X_train['NumberOfTimes90DaysLate']\n",
    "\n",
    "clf = RandomForestClassifier(max_depth = 10, max_features ='auto', random_state = 0)\n",
    "clf_SFS = SequentialFeatureSelector(clf, n_features_to_select=10, direction='forward', scoring='roc_auc', cv=3, n_jobs=-1)\n",
    "# clf = RFE(clf, n_features_to_select=3, step=1)\n",
    "clf_SFS.fit(X_train, y_train)\n",
    "X_selected = X_train.iloc[:, clf_SFS.get_support()]\n",
    "\n",
    "clf.fit(X_selected, y_train)\n",
    "print('ROC_AUC score:', roc_auc_score(y_train, clf.predict(X_selected)))\n",
    "\n",
    "# now let's draw ROC curve\n",
    "plt.figure(figsize=[9, 6])\n",
    "fpr, tpr, _ = roc_curve(y_train, clf.predict_proba(X_selected)[:, 1])\n",
    "plt.plot(fpr, tpr, 'r', label='train')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()\n",
    "\n",
    "# feature importances\n",
    "fi = pd.Series(clf.feature_importances_, index=X_selected.columns)\n",
    "fi.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('feature importances')\n",
    "# plt.ylim([0, 0.2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Trees_Bagging_Random Forest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
